{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c78b862-dc25-42a1-af05-578f30fb6d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 12)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('research_traffic_multiple_labels.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3482678-3b7a-4e5e-9ebf-3ac0c365436e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Source IP        int64\n",
      "Port Count                int64\n",
      "Pair Count Ratio        float64\n",
      "Packet Count Diff         int64\n",
      "Lookup Count Diff         int64\n",
      "Protocol                  int64\n",
      "Average Packet Count    float64\n",
      "Average Byte Count      float64\n",
      "Packet Std Dev          float64\n",
      "Byte Std Dev            float64\n",
      "Duration per Flow       float64\n",
      "Label                     int64\n",
      "dtype: object\n",
      "10000\n",
      "10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count of Source IP</th>\n",
       "      <th>Port Count</th>\n",
       "      <th>Pair Count Ratio</th>\n",
       "      <th>Packet Count Diff</th>\n",
       "      <th>Lookup Count Diff</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Average Packet Count</th>\n",
       "      <th>Average Byte Count</th>\n",
       "      <th>Packet Std Dev</th>\n",
       "      <th>Byte Std Dev</th>\n",
       "      <th>Duration per Flow</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>4.788991</td>\n",
       "      <td>0.163601</td>\n",
       "      <td>28.466647</td>\n",
       "      <td>0.668020</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>1.596330</td>\n",
       "      <td>0.095342</td>\n",
       "      <td>16.589551</td>\n",
       "      <td>0.419081</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018792</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.023853</td>\n",
       "      <td>4.150459</td>\n",
       "      <td>0.355016</td>\n",
       "      <td>61.772825</td>\n",
       "      <td>2.946281</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>4.788991</td>\n",
       "      <td>0.163601</td>\n",
       "      <td>28.466647</td>\n",
       "      <td>2.674461</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Count of Source IP  Port Count  Pair Count Ratio  Packet Count Diff  \\\n",
       "0                 545         545               0.0                  2   \n",
       "1                 545         545               0.0                 10   \n",
       "2                  52          52               0.0                  3   \n",
       "3                 545         545               0.0                 13   \n",
       "4                 545         545               0.0                  2   \n",
       "\n",
       "   Lookup Count Diff  Protocol  Average Packet Count  Average Byte Count  \\\n",
       "0                 43         6              0.027523            4.788991   \n",
       "1                  0         6              0.009174            1.596330   \n",
       "2                  0         6              0.000000            0.000000   \n",
       "3                 21         6              0.023853            4.150459   \n",
       "4                131         6              0.027523            4.788991   \n",
       "\n",
       "   Packet Std Dev  Byte Std Dev  Duration per Flow  Label  \n",
       "0        0.163601     28.466647           0.668020      3  \n",
       "1        0.095342     16.589551           0.419081      3  \n",
       "2        0.000000      0.000000           0.018792      3  \n",
       "3        0.355016     61.772825           2.946281      3  \n",
       "4        0.163601     28.466647           2.674461      3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.dtypes)\n",
    "print(df['Label'].value_counts()[1])\n",
    "print(df['Label'].value_counts()[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e4c5b01-2c83-421a-a31a-dad6da3377ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_cols=df.dtypes[df.dtypes == \"object\"].index.values.tolist()\n",
    "# print(obj_cols)\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# #Encode labels of multiple columns at once\n",
    "\n",
    "# df[obj_cols] = df[obj_cols].astype(str)\n",
    "# df[obj_cols] = df[obj_cols].apply(LabelEncoder().fit_transform)\n",
    "# #\n",
    "# # Print head\n",
    "# #\n",
    "# print(df.dtypes)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a088bc65-ad53-4303-b9ef-b7c911b0b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# import lightgbm as lgb\n",
    "\n",
    "# y = df.Label\n",
    "# X = df.drop(['Label'],axis=1)\n",
    "\n",
    "# # Scale the dataset using StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "# X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# params = {\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'objective': 'binary',\n",
    "#     'metric': 'binary_logloss',\n",
    "#     'num_leaves': 31,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'feature_fraction': 0.9,\n",
    "#     'bagging_fraction': 0.8,\n",
    "#     'bagging_freq': 5,\n",
    "#     'verbose': 0\n",
    "# }\n",
    "\n",
    "\n",
    "# # Setting up 5-fold cross-validation\n",
    "# kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# fold = 0\n",
    "# results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f12cf1-76b3-40cb-a4dc-bf4a0c086b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset normalized using MinMaxScaler.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_score, make_scorer\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, KFold, cross_validate\n",
    "from datetime import datetime\n",
    "\n",
    "y = df.Label\n",
    "X = df.drop(['Label'],axis=1)\n",
    "\n",
    "# Normalize the dataset using MinMaxScaler\n",
    "normalizer = MinMaxScaler()\n",
    "X_normalized = normalizer.fit_transform(X)\n",
    "X = pd.DataFrame(X_normalized, columns=X.columns)\n",
    "print(\"Dataset normalized using MinMaxScaler.\")\n",
    "\n",
    "# # Scale the dataset using StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "# X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "X_train_cv, X_unseen_test, y_train_cv, y_unseen_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52e8ce7-88f3-40ce-bf16-704976068283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2445\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.787601\n",
      "[LightGBM] [Info] Start training from score -1.802371\n",
      "[LightGBM] [Info] Start training from score -1.788709\n",
      "[LightGBM] [Info] Start training from score -1.791482\n",
      "[LightGBM] [Info] Start training from score -1.787878\n",
      "[LightGBM] [Info] Start training from score -1.792593\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2450\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.797051\n",
      "[LightGBM] [Info] Start training from score -1.796353\n",
      "[LightGBM] [Info] Start training from score -1.794123\n",
      "[LightGBM] [Info] Start training from score -1.794263\n",
      "[LightGBM] [Info] Start training from score -1.784839\n",
      "[LightGBM] [Info] Start training from score -1.784012\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2443\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.791621\n",
      "[LightGBM] [Info] Start training from score -1.806168\n",
      "[LightGBM] [Info] Start training from score -1.788847\n",
      "[LightGBM] [Info] Start training from score -1.795795\n",
      "[LightGBM] [Info] Start training from score -1.782635\n",
      "[LightGBM] [Info] Start training from score -1.785667\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2446\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.796214\n",
      "[LightGBM] [Info] Start training from score -1.793845\n",
      "[LightGBM] [Info] Start training from score -1.798169\n",
      "[LightGBM] [Info] Start training from score -1.788432\n",
      "[LightGBM] [Info] Start training from score -1.787325\n",
      "[LightGBM] [Info] Start training from score -1.786634\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001334 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2444\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.794820\n",
      "[LightGBM] [Info] Start training from score -1.798868\n",
      "[LightGBM] [Info] Start training from score -1.791343\n",
      "[LightGBM] [Info] Start training from score -1.791204\n",
      "[LightGBM] [Info] Start training from score -1.786219\n",
      "[LightGBM] [Info] Start training from score -1.788155\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2441\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.798029\n",
      "[LightGBM] [Info] Start training from score -1.790926\n",
      "[LightGBM] [Info] Start training from score -1.794263\n",
      "[LightGBM] [Info] Start training from score -1.790926\n",
      "[LightGBM] [Info] Start training from score -1.784701\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2445\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.789817\n",
      "[LightGBM] [Info] Start training from score -1.798029\n",
      "[LightGBM] [Info] Start training from score -1.794263\n",
      "[LightGBM] [Info] Start training from score -1.792454\n",
      "[LightGBM] [Info] Start training from score -1.786357\n",
      "[LightGBM] [Info] Start training from score -1.789678\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2450\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.793706\n",
      "[LightGBM] [Info] Start training from score -1.796772\n",
      "[LightGBM] [Info] Start training from score -1.793845\n",
      "[LightGBM] [Info] Start training from score -1.791898\n",
      "[LightGBM] [Info] Start training from score -1.788155\n",
      "[LightGBM] [Info] Start training from score -1.786219\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2443\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.792593\n",
      "[LightGBM] [Info] Start training from score -1.800968\n",
      "[LightGBM] [Info] Start training from score -1.789263\n",
      "[LightGBM] [Info] Start training from score -1.792593\n",
      "[LightGBM] [Info] Start training from score -1.789124\n",
      "[LightGBM] [Info] Start training from score -1.786081\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2432\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.793706\n",
      "[LightGBM] [Info] Start training from score -1.796493\n",
      "[LightGBM] [Info] Start training from score -1.794402\n",
      "[LightGBM] [Info] Start training from score -1.790233\n",
      "[LightGBM] [Info] Start training from score -1.790510\n",
      "[LightGBM] [Info] Start training from score -1.785253\n",
      "excution time:  0:00:09.079613\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store metrics\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "# Initialize LightGBM classifier\n",
    "lgb_clf = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for train_index, test_index in kf.split(X_train_cv):\n",
    "    X_train, X_test = X_train_cv.iloc[train_index], X_train_cv.iloc[test_index]\n",
    "    y_train, y_test = y_train_cv.iloc[train_index], y_train_cv.iloc[test_index]\n",
    "\n",
    "    # Train the LightGBM classifier\n",
    "    lgb_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test fold\n",
    "    y_pred_test = lgb_clf.predict(X_test)\n",
    "    y_pred_train = lgb_clf.predict(X_train)\n",
    "\n",
    "    # Calculate metrics for the test fold\n",
    "    train_accuracies.append(accuracy_score(y_train, y_pred_train))\n",
    "    test_accuracies.append(accuracy_score(y_test, y_pred_test))\n",
    "    precisions.append(precision_score(y_test, y_pred_test, average='weighted'))\n",
    "    recalls.append(recall_score(y_test, y_pred_test, average='weighted'))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_test, average='weighted'))\n",
    "    confusion_matrices.append(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "# Evaluate on unseen test data\n",
    "y_unseen_pred = lgb_clf.predict(X_unseen_test)\n",
    "unseen_accuracy = accuracy_score(y_unseen_test, y_unseen_pred)\n",
    "unseen_precision = precision_score(y_unseen_test, y_unseen_pred, average='weighted')\n",
    "unseen_recall = recall_score(y_unseen_test, y_unseen_pred, average='weighted')\n",
    "unseen_f1 = f1_score(y_unseen_test, y_unseen_pred, average='weighted')\n",
    "unseen_conf_matrix = confusion_matrix(y_unseen_test, y_unseen_pred)\n",
    "\n",
    "end = datetime.now()\n",
    "print(\"excution time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d39327f8-be2e-436d-b055-9381cdaeb2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Confusion Matrix:\n",
      "625.70  32.60   0.20  34.00   0.20 106.40\n",
      "  5.50 779.40   0.10   0.00   0.20   9.20\n",
      "  0.80   0.40 798.00   0.00   0.00   0.30\n",
      "  2.50   0.00   0.00 796.90   0.00   0.20\n",
      "  1.50   0.00   0.00   0.00 802.00   0.00\n",
      " 17.00   4.60   0.10   0.00   0.00 782.20\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average confusion matrix for multi-class classification\n",
    "import numpy as np\n",
    "\n",
    "# Determine the number of classes from the confusion matrices\n",
    "num_classes = confusion_matrices[0].shape[0]\n",
    "\n",
    "# Initialize an array to store the sum of confusion matrices\n",
    "confusion_matrix_sum = np.zeros((num_classes, num_classes))\n",
    "\n",
    "# Sum up the confusion matrices from all folds\n",
    "for cm in confusion_matrices:\n",
    "    confusion_matrix_sum += cm\n",
    "\n",
    "# Compute the average confusion matrix\n",
    "average_conf_matrix = confusion_matrix_sum / len(confusion_matrices)\n",
    "\n",
    "# Display the average confusion matrix\n",
    "print(\"Average Confusion Matrix:\")\n",
    "max_width = max(len(\"{:.2f}\".format(value)) for row in average_conf_matrix for value in row)\n",
    "\n",
    "# Print each row with formatted values\n",
    "for row in average_conf_matrix:\n",
    "    print(\" \".join(f\"{value:>{max_width}.2f}\" for value in row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a1e2695-cc6d-4363-a39c-0bc143c6b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Training Accuracy: 0.9687962962962963\n",
      "Testing Accuracy: 0.9575\n",
      "Precision: 0.9589394499436584\n",
      "Recall: 0.9575\n",
      "F1-Score: 0.9562175861121932\n",
      "Confusion Matrix:\n",
      "[[592  39   0  27   0 103]\n",
      " [  4 810   0   0   0   6]\n",
      " [  2   0 771   0   0   0]\n",
      " [  1   0   0 793   0   0]\n",
      " [  3   0   0   0 804   0]\n",
      " [ 16   3   0   0   0 826]]\n",
      "\n",
      "Fold 2:\n",
      "Training Accuracy: 0.9692361111111111\n",
      "Testing Accuracy: 0.9497916666666667\n",
      "Precision: 0.9526863462891919\n",
      "Recall: 0.9497916666666667\n",
      "F1-Score: 0.9484792097067968\n",
      "Confusion Matrix:\n",
      "[[634  30   0  42   1 122]\n",
      " [  7 754   1   0   0  15]\n",
      " [  0   0 811   0   0   1]\n",
      " [  0   0   0 814   0   0]\n",
      " [  2   0   0   0 783   0]\n",
      " [ 17   3   0   0   0 763]]\n",
      "\n",
      "Fold 3:\n",
      "Training Accuracy: 0.9689351851851852\n",
      "Testing Accuracy: 0.9564583333333333\n",
      "Precision: 0.9585160149409956\n",
      "Recall: 0.9564583333333333\n",
      "F1-Score: 0.955493997802997\n",
      "Confusion Matrix:\n",
      "[[625  28   0  29   0 108]\n",
      " [  7 830   0   0   0  10]\n",
      " [  0   1 773   0   0   0]\n",
      " [  4   0   0 820   0   1]\n",
      " [  1   0   0   0 768   0]\n",
      " [ 11   9   0   0   0 775]]\n",
      "\n",
      "Fold 4:\n",
      "Training Accuracy: 0.9690277777777778\n",
      "Testing Accuracy: 0.95375\n",
      "Precision: 0.9554979162968766\n",
      "Recall: 0.95375\n",
      "F1-Score: 0.9523751252470141\n",
      "Confusion Matrix:\n",
      "[[640  37   0  43   1 102]\n",
      " [  8 746   0   0   0   5]\n",
      " [  1   1 839   0   0   0]\n",
      " [  0   0   0 772   0   0]\n",
      " [  1   0   0   0 802   0]\n",
      " [ 15   8   0   0   0 779]]\n",
      "\n",
      "Fold 5:\n",
      "Training Accuracy: 0.9688657407407407\n",
      "Testing Accuracy: 0.95625\n",
      "Precision: 0.9585547143243834\n",
      "Recall: 0.95625\n",
      "F1-Score: 0.9551239895663785\n",
      "Confusion Matrix:\n",
      "[[639  38   0  28   0 108]\n",
      " [  1 782   0   0   1  11]\n",
      " [  0   1 791   0   0   0]\n",
      " [  4   0   0 788   0   0]\n",
      " [  0   0   0   0 795   0]\n",
      " [ 14   4   0   0   0 795]]\n",
      "\n",
      "Fold 6:\n",
      "Training Accuracy: 0.9688194444444445\n",
      "Testing Accuracy: 0.9554166666666667\n",
      "Precision: 0.9568060384327683\n",
      "Recall: 0.9554166666666667\n",
      "F1-Score: 0.9542175046505936\n",
      "Confusion Matrix:\n",
      "[[620  38   0  36   0  97]\n",
      " [  5 777   0   0   0   7]\n",
      " [  3   0 786   0   0   0]\n",
      " [  2   0   0 811   0   1]\n",
      " [  2   0   0   0 827   0]\n",
      " [ 17   6   0   0   0 765]]\n",
      "\n",
      "Fold 7:\n",
      "Training Accuracy: 0.9686574074074074\n",
      "Testing Accuracy: 0.954375\n",
      "Precision: 0.9555189773541181\n",
      "Recall: 0.954375\n",
      "F1-Score: 0.9533457121359301\n",
      "Confusion Matrix:\n",
      "[[612  31   2  29   0 103]\n",
      " [  4 772   0   0   0  13]\n",
      " [  0   0 812   0   0   1]\n",
      " [  3   0   0 798   0   0]\n",
      " [  0   0   0   0 796   0]\n",
      " [ 28   4   1   0   0 791]]\n",
      "\n",
      "Fold 8:\n",
      "Training Accuracy: 0.9688194444444445\n",
      "Testing Accuracy: 0.9566666666666667\n",
      "Precision: 0.9579215667722597\n",
      "Recall: 0.9566666666666667\n",
      "F1-Score: 0.9555781856919775\n",
      "Confusion Matrix:\n",
      "[[640  34   0  42   0  89]\n",
      " [  6 764   0   0   0  10]\n",
      " [  2   1 806   0   0   1]\n",
      " [  1   0   0 796   0   0]\n",
      " [  0   0   0   0 809   0]\n",
      " [ 20   2   0   0   0 777]]\n",
      "\n",
      "Fold 9:\n",
      "Training Accuracy: 0.9684259259259259\n",
      "Testing Accuracy: 0.956875\n",
      "Precision: 0.9583364536414676\n",
      "Recall: 0.956875\n",
      "F1-Score: 0.9560635606911763\n",
      "Confusion Matrix:\n",
      "[[639  22   0  30   0 106]\n",
      " [  7 792   0   0   1  10]\n",
      " [  0   0 777   0   0   0]\n",
      " [  8   0   0 794   0   0]\n",
      " [  1   0   0   0 815   0]\n",
      " [ 18   4   0   0   0 776]]\n",
      "\n",
      "Fold 10:\n",
      "Training Accuracy: 0.968912037037037\n",
      "Testing Accuracy: 0.9533333333333334\n",
      "Precision: 0.9558028558402534\n",
      "Recall: 0.9533333333333334\n",
      "F1-Score: 0.951984081054974\n",
      "Confusion Matrix:\n",
      "[[616  29   0  34   0 126]\n",
      " [  6 767   0   0   0   5]\n",
      " [  0   0 814   0   0   0]\n",
      " [  2   0   0 783   0   0]\n",
      " [  5   0   0   0 821   0]\n",
      " [ 14   3   0   0   0 775]]\n",
      "\n",
      "Average Training Accuracy: 0.968849537037037\n",
      "Average Testing Accuracy: 0.9550416666666667\n",
      "Average Precision: 0.9568580333835974\n",
      "Average recall: 0.9550416666666666\n",
      "Average F1 Score: 0.9538878952660029\n",
      "Performance on Unseen Data:\n",
      "Accuracy: 0.95475\n",
      "Precision: 0.9565394471248935\n",
      "Recall: 0.95475\n",
      "F1-Score: 0.9535446748169473\n",
      "Confusion Matrix:\n",
      "[[1569   82    2   90    2  264]\n",
      " [  12 2021    0    0    0   23]\n",
      " [   2    0 2003    0    0    0]\n",
      " [   7    0    0 1997    0    0]\n",
      " [   5    0    0    0 1960    0]\n",
      " [  41   13    0    0    0 1907]]\n"
     ]
    }
   ],
   "source": [
    "# Print the metrics for each fold\n",
    "for i in range(10):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"Training Accuracy: {train_accuracies[i]}\")\n",
    "    print(f\"Testing Accuracy: {test_accuracies[i]}\")\n",
    "    print(f\"Precision: {precisions[i]}\")\n",
    "    print(f\"Recall: {recalls[i]}\")\n",
    "    print(f\"F1-Score: {f1_scores[i]}\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrices[i]}\\n\")\n",
    "\n",
    "print(f\"Average Training Accuracy: {sum(train_accuracies) / 10}\")\n",
    "print(f\"Average Testing Accuracy: {sum(test_accuracies) / 10}\")\n",
    "print(f\"Average Precision: {sum(precisions) / 10}\")\n",
    "print(f\"Average recall: {sum(recalls) / 10}\")\n",
    "print(f\"Average F1 Score: {sum(f1_scores) / 10}\")\n",
    "\n",
    "# Print the performance on the unseen data\n",
    "print(\"Performance on Unseen Data:\")\n",
    "print(f\"Accuracy: {unseen_accuracy}\")\n",
    "print(f\"Precision: {unseen_precision}\")\n",
    "print(f\"Recall: {unseen_recall}\")\n",
    "print(f\"F1-Score: {unseen_f1}\")\n",
    "print(f\"Confusion Matrix:\\n{unseen_conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30a4ea85-cc4d-461e-a291-470f553be27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weighted Feature Importance (WFI):\n",
      "                 Feature  Importance\n",
      "10     Duration per Flow    0.201333\n",
      "4      Lookup Count Diff    0.157000\n",
      "3      Packet Count Diff    0.156556\n",
      "6   Average Packet Count    0.094278\n",
      "0     Count of Source IP    0.090833\n",
      "1             Port Count    0.067722\n",
      "8         Packet Std Dev    0.062444\n",
      "7     Average Byte Count    0.055611\n",
      "2       Pair Count Ratio    0.043222\n",
      "9           Byte Std Dev    0.039667\n",
      "5               Protocol    0.031333\n"
     ]
    }
   ],
   "source": [
    "# Calculate Weighted Feature Importance (WFI)\n",
    "feature_importances = lgb_clf.feature_importances_\n",
    "wfi = feature_importances / feature_importances.sum()\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': wfi\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nWeighted Feature Importance (WFI):\")\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edbfdaf1-be14-4b4d-8ef1-aa4930754553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('lgb_multi.pkl', 'wb') as file:\n",
    "#     pickle.dump(lgb_clf, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
