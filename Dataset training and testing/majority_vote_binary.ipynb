{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e869096d-6afe-426b-bb90-ba8168f6af8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140000, 12)\n",
      "70000\n",
      "70000\n",
      "       Count_of_Source_IP  Port_Count  Pair_Count_Ratio  Packet_Count_Diff  \\\n",
      "40665                 545         545               0.0                  1   \n",
      "\n",
      "       Lookup_Count_Diff  Protocol  Average_Packet_Count  Average_Byte_Count  \\\n",
      "40665                  0        17                   0.0                 0.0   \n",
      "\n",
      "       Packet_Std_Dev  Byte_Std_Dev  Duration_per_Flow  Label  \n",
      "40665             0.0           0.0           0.365574      1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "df = pd.read_csv('research_traffic.csv')\n",
    "print(df.shape)\n",
    "print(df['Label'].value_counts()[1])\n",
    "print(df['Label'].value_counts()[0])\n",
    "df.head()\n",
    "# df = df.iloc[[40665]]\n",
    "# df.head()\n",
    "print(df.iloc[[40665]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41c43163-419c-4c0d-bb63-e74c9bc93147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset normalized using MinMaxScaler.\n",
      "Data normalized and split.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold, cross_validate\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, classification_report\n",
    ")\n",
    "from datetime import datetime\n",
    "\n",
    "y = df.Label\n",
    "X = df.drop(['Label'],axis=1)\n",
    "\n",
    "# Normalize the dataset using MinMaxScaler\n",
    "normalizer = MinMaxScaler()\n",
    "X_normalized = normalizer.fit_transform(X)\n",
    "X = pd.DataFrame(X_normalized, columns=X.columns)\n",
    "print(\"Dataset normalized using MinMaxScaler.\")\n",
    "\n",
    "X_train_cv, X_unseen_test, y_train_cv, y_unseen_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "print(\"Data normalized and split.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d164583-7fc3-407c-8581-53f1d1fe7709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "start = datetime.now()\n",
    "pickle_files = [\"ann_binary.pkl\", \"knn_binary.pkl\", \"xgb_binary.pkl\", \"dt_binary.pkl\", \"nb_binary.pkl\", \"lgb_binary.pkl\", \"lr_binary.pkl\", \"rf_binary.pkl\", \"svm_binary.pkl\"]\n",
    "for filename in pickle_files:\n",
    "    with open(filename, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    model_name = filename.replace(\"_binary.pkl\", \"\")\n",
    "    models.append((model_name, model))\n",
    "print(f\"{len(models)} models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d1058d-b6a4-464a-b1ec-4d7cb6363a30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 940us/step\n",
      "Results for model: ann\n",
      "Accuracy:  0.9839285714285714\n",
      "Precision: 0.9761391304347826\n",
      "Recall:    0.9924322795105736\n",
      "F1 Score:  0.9842182787402679\n",
      "Confusion Matrix:\n",
      "[[13518   343]\n",
      " [  107 14032]]\n",
      "----------------------------------------\n",
      "Results for model: knn\n",
      "Accuracy:  0.98275\n",
      "Precision: 0.9770821688093907\n",
      "Recall:    0.9890374142442888\n",
      "F1 Score:  0.9830234438156831\n",
      "Confusion Matrix:\n",
      "[[13533   328]\n",
      " [  155 13984]]\n",
      "----------------------------------------\n",
      "Results for model: xgb\n",
      "Accuracy:  0.9868571428571429\n",
      "Precision: 0.9785252623531865\n",
      "Recall:    0.9958271447768583\n",
      "F1 Score:  0.9871003925967471\n",
      "Confusion Matrix:\n",
      "[[13552   309]\n",
      " [   59 14080]]\n",
      "----------------------------------------\n",
      "Results for model: dt\n",
      "Accuracy:  0.9785357142857143\n",
      "Precision: 0.978442182640656\n",
      "Recall:    0.9790649975245774\n",
      "F1 Score:  0.9787534910029342\n",
      "Confusion Matrix:\n",
      "[[13556   305]\n",
      " [  296 13843]]\n",
      "----------------------------------------\n",
      "Results for model: nb\n",
      "Accuracy:  0.9311428571428572\n",
      "Precision: 0.8873310917972467\n",
      "Recall:    0.9892495933234317\n",
      "F1 Score:  0.9355227075112033\n",
      "Confusion Matrix:\n",
      "[[12085  1776]\n",
      " [  152 13987]]\n",
      "----------------------------------------\n",
      "Results for model: lgb\n",
      "Accuracy:  0.9865357142857143\n",
      "Precision: 0.978112840466926\n",
      "Recall:    0.9956149656977156\n",
      "F1 Score:  0.9867863026182048\n",
      "Confusion Matrix:\n",
      "[[13546   315]\n",
      " [   62 14077]]\n",
      "----------------------------------------\n",
      "Results for model: lr\n",
      "Accuracy:  0.9755357142857143\n",
      "Precision: 0.9762140733399405\n",
      "Recall:    0.9753165004597213\n",
      "F1 Score:  0.9757650804882363\n",
      "Confusion Matrix:\n",
      "[[13525   336]\n",
      " [  349 13790]]\n",
      "----------------------------------------\n",
      "Results for model: rf\n",
      "Accuracy:  0.9875\n",
      "Precision: 0.9798176630245667\n",
      "Recall:    0.995756418417144\n",
      "F1 Score:  0.987722744492774\n",
      "Confusion Matrix:\n",
      "[[13571   290]\n",
      " [   60 14079]]\n",
      "----------------------------------------\n",
      "Results for model: svm\n",
      "Accuracy:  0.9808214285714286\n",
      "Precision: 0.9760604787904242\n",
      "Recall:    0.9862083598557182\n",
      "F1 Score:  0.9811081794195251\n",
      "Confusion Matrix:\n",
      "[[13519   342]\n",
      " [  195 13944]]\n",
      "----------------------------------------\n",
      "Predictions obtained from all models.\n",
      "Majority Vote Ensemble Evaluation (Pandas Mode):\n",
      "Accuracy:  0.9851071428571428\n",
      "Precision: 0.9772537562604341\n",
      "Recall:    0.9936346276257161\n",
      "F1 Score:  0.9853761178327196\n",
      "Confusion Matrix:\n",
      " [[13534   327]\n",
      " [   90 14049]]\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "for name, model in models:\n",
    "    if name == \"ann\":\n",
    "        prediction = np.array((model.predict(X_unseen_test) > 0.5).astype(int))\n",
    "    else:\n",
    "        prediction = np.array(model.predict(X_unseen_test))\n",
    "    if prediction.ndim > 1:\n",
    "        prediction = prediction.ravel()\n",
    "    predictions[name] = prediction\n",
    "\n",
    "    acc = accuracy_score(y_unseen_test, prediction)\n",
    "    prec = precision_score(y_unseen_test, prediction)\n",
    "    rec = recall_score(y_unseen_test, prediction)\n",
    "    f1  = f1_score(y_unseen_test, prediction)\n",
    "    cm  = confusion_matrix(y_unseen_test, prediction)\n",
    "\n",
    "    # Print the results for the current model\n",
    "    print(f\"Results for model: {name}\")\n",
    "    print(\"Accuracy: \", acc)\n",
    "    print(\"Precision:\", prec)\n",
    "    print(\"Recall:   \", rec)\n",
    "    print(\"F1 Score: \", f1)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"-\" * 40)\n",
    "# # Optional: Inspect one of the predictions\n",
    "# print(predictions['ann'])\n",
    "# Step 2: Combine predictions and apply majority vote ensemble using pandas mode\n",
    "# Create a DataFrame where each column corresponds to a model's predictions\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "pred_df.head()\n",
    "\n",
    "print(\"Predictions obtained from all models.\")\n",
    "\n",
    "# For each sample (row), get the mode (most common value) across the classifiers.\n",
    "# Since there are 9 classifiers (an odd number), ties should not occur.\n",
    "ensemble_predictions = pred_df.mode(axis=1)[0].values\n",
    "\n",
    "# Step 3: Evaluate the ensemble predictions using standard metrics\n",
    "accuracy  = accuracy_score(y_unseen_test, ensemble_predictions)\n",
    "precision = precision_score(y_unseen_test, ensemble_predictions)\n",
    "recall    = recall_score(y_unseen_test, ensemble_predictions)\n",
    "f1        = f1_score(y_unseen_test, ensemble_predictions)\n",
    "cm        = confusion_matrix(y_unseen_test, ensemble_predictions)\n",
    "\n",
    "# Print out the results\n",
    "print(\"Majority Vote Ensemble Evaluation (Pandas Mode):\")\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:   \", recall)\n",
    "print(\"F1 Score: \", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2ca0da-28fe-4f32-9d2e-8ee65ae734c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the objective function to maximize accuracy\n",
    "def evaluate(weights):\n",
    "    weighted_predictions = []\n",
    "    for i in range(len(y_unseen_test)):\n",
    "        class_votes = {}\n",
    "        for j, (name, model) in enumerate(models):\n",
    "            pred = predictions[name][i]\n",
    "            class_votes[pred] = class_votes.get(pred, 0) + weights[j]\n",
    "        weighted_predictions.append(max(class_votes, key=class_votes.get))\n",
    "    acc = accuracy_score(y_unseen_test, weighted_predictions)\n",
    "    return (acc,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc991a0-2d30-49b7-bb73-6016d5f51e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t50    \n",
      "1  \t35    \n",
      "2  \t20    \n",
      "3  \t33    \n",
      "4  \t35    \n",
      "5  \t30    \n",
      "6  \t30    \n",
      "7  \t29    \n",
      "8  \t32    \n",
      "9  \t33    \n",
      "10 \t29    \n",
      "11 \t32    \n",
      "12 \t25    \n",
      "13 \t29    \n",
      "14 \t25    \n",
      "15 \t18    \n",
      "16 \t24    \n",
      "17 \t30    \n",
      "18 \t31    \n",
      "19 \t40    \n",
      "20 \t30    \n",
      "21 \t34    \n",
      "22 \t34    \n",
      "23 \t32    \n",
      "24 \t31    \n",
      "25 \t27    \n",
      "26 \t29    \n",
      "27 \t31    \n",
      "28 \t28    \n",
      "29 \t35    \n",
      "30 \t27    \n",
      "31 \t30    \n",
      "32 \t16    \n",
      "33 \t28    \n",
      "34 \t27    \n",
      "35 \t30    \n",
      "36 \t29    \n",
      "37 \t34    \n",
      "38 \t30    \n",
      "39 \t41    \n",
      "40 \t29    \n",
      "41 \t42    \n",
      "42 \t23    \n",
      "43 \t28    \n",
      "44 \t30    \n",
      "45 \t37    \n",
      "46 \t24    \n",
      "47 \t27    \n",
      "48 \t29    \n",
      "49 \t38    \n",
      "50 \t28    \n",
      "Optimized Classifier Weights: {'ann': 0.0007125540412833424, 'knn': 0.11223164100871337, 'xgb': 0.32267916146757, 'dt': 0.35360769610060067, 'nb': -0.05292927734128819, 'lgb': -0.06984599326363714, 'lr': -0.2580602524956523, 'rf': 0.5236689154112776, 'svm': 0.06793555507113266}\n"
     ]
    }
   ],
   "source": [
    "# Set up the Genetic Algorithm\n",
    "num_models = len(models)\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Maximize accuracy\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Define population and individual initialization\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_float\", np.random.rand)  # Random weight initialization\n",
    "\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=num_models)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Genetic operators\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.2, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Run Genetic Algorithm\n",
    "pop = toolbox.population(n=50)  # Population size\n",
    "algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=50, verbose=True)\n",
    "\n",
    "# Extract best weights\n",
    "best_individual = np.array(tools.selBest(pop, k=1)[0])\n",
    "optimized_weights = best_individual / np.sum(best_individual)  # Normalize weights\n",
    "\n",
    "# Store classifier names with optimized weights in a dictionary\n",
    "classifier_names = [name for name, _ in models]\n",
    "classifier_weights = {clf_name: weight for clf_name, weight in zip(classifier_names, optimized_weights)}\n",
    "\n",
    "print(\"Optimized Classifier Weights:\", classifier_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "134e6eb5-9418-4eb5-aa11-e3bdc67dacc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_weights = {'ann': 0.04337417816855901, 'knn': 0.005602240325772464, 'xgb': 0.4703796361375013, 'dt': 0.436420021927582, 'nb': -0.35000903676225836, 'lgb': 0.03877736424779778, 'lr': -0.14712137662935784, 'rf': 0.6058174300103625, 'svm': -0.10324045742595897}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e27f673-22be-4853-a572-78418dc0d1c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Ensemble Evaluation:\n",
      "Accuracy:  0.98775\n",
      "Precision: 0.9798942535132878\n",
      "Recall:    0.9961807765754297\n",
      "F1 Score:  0.9879703994669098\n",
      "Confusion Matrix:\n",
      " [[13572   289]\n",
      " [   54 14085]]\n",
      "training time:  0:05:45.296030\n"
     ]
    }
   ],
   "source": [
    "# List to store the final weighted ensemble predictions\n",
    "weighted_ensemble_predictions = []\n",
    "n_samples = len(y_unseen_test)\n",
    "\n",
    "# Loop over each sample in the test set\n",
    "for i in range(n_samples):\n",
    "    weighted_votes = {}\n",
    "\n",
    "    for name, prediction in predictions.items():\n",
    "        pred_class = prediction[i]\n",
    "        weight = classifier_weights.get(name, 1.0)\n",
    "        # Accumulate the vote for the predicted class\n",
    "        weighted_votes[pred_class] = weighted_votes.get(pred_class, 0) + weight\n",
    "\n",
    "    final_class = max(weighted_votes, key=weighted_votes.get)\n",
    "    weighted_ensemble_predictions.append(final_class)\n",
    "\n",
    "end = datetime.now()\n",
    "\n",
    "accuracy  = accuracy_score(y_unseen_test, weighted_ensemble_predictions)\n",
    "precision = precision_score(y_unseen_test, weighted_ensemble_predictions)\n",
    "recall    = recall_score(y_unseen_test, weighted_ensemble_predictions)\n",
    "f1        = f1_score(y_unseen_test, weighted_ensemble_predictions)\n",
    "cm        = confusion_matrix(y_unseen_test, weighted_ensemble_predictions)\n",
    "\n",
    "\n",
    "print(\"Weighted Ensemble Evaluation:\")\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:   \", recall)\n",
    "print(\"F1 Score: \", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"training time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c82c58-95b8-43fb-ae81-eda528f56b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
