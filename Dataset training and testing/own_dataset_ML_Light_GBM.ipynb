{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c78b862-dc25-42a1-af05-578f30fb6d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 12)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('research_traffic.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3482678-3b7a-4e5e-9ebf-3ac0c365436e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count_of_Source_IP        int64\n",
      "Port_Count                int64\n",
      "Pair_Count_Ratio        float64\n",
      "Packet_Count_Diff         int64\n",
      "Lookup_Count_Diff         int64\n",
      "Protocol                  int64\n",
      "Average_Packet_Count    float64\n",
      "Average_Byte_Count      float64\n",
      "Packet_Std_Dev          float64\n",
      "Byte_Std_Dev            float64\n",
      "Duration_per_Flow       float64\n",
      "Label                     int64\n",
      "dtype: object\n",
      "70000\n",
      "70000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count_of_Source_IP</th>\n",
       "      <th>Port_Count</th>\n",
       "      <th>Pair_Count_Ratio</th>\n",
       "      <th>Packet_Count_Diff</th>\n",
       "      <th>Lookup_Count_Diff</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Average_Packet_Count</th>\n",
       "      <th>Average_Byte_Count</th>\n",
       "      <th>Packet_Std_Dev</th>\n",
       "      <th>Byte_Std_Dev</th>\n",
       "      <th>Duration_per_Flow</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>436913</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>145637.6667</td>\n",
       "      <td>3.567915e+09</td>\n",
       "      <td>24719.5</td>\n",
       "      <td>5.339086e+09</td>\n",
       "      <td>1.258000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>436912</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>145637.3333</td>\n",
       "      <td>3.567915e+09</td>\n",
       "      <td>24721.0</td>\n",
       "      <td>5.339086e+09</td>\n",
       "      <td>1.254000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1006277</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>335425.6667</td>\n",
       "      <td>8.291520e+09</td>\n",
       "      <td>61289.5</td>\n",
       "      <td>1.240812e+10</td>\n",
       "      <td>2.587333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1006280</td>\n",
       "      <td>6</td>\n",
       "      <td>335426.0000</td>\n",
       "      <td>8.291520e+09</td>\n",
       "      <td>61288.0</td>\n",
       "      <td>1.240812e+10</td>\n",
       "      <td>2.592000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1605562</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>535187.3333</td>\n",
       "      <td>1.321503e+10</td>\n",
       "      <td>97006.0</td>\n",
       "      <td>1.977597e+10</td>\n",
       "      <td>3.926000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Count_of_Source_IP  Port_Count  Pair_Count_Ratio  Packet_Count_Diff  \\\n",
       "0                   2           2               1.0             436913   \n",
       "1                   2           2               1.0             436912   \n",
       "2                   2           2               1.0            1006277   \n",
       "3                   2           2               0.0                  1   \n",
       "4                   2           2               1.0            1605562   \n",
       "\n",
       "   Lookup_Count_Diff  Protocol  Average_Packet_Count  Average_Byte_Count  \\\n",
       "0                  1         6           145637.6667        3.567915e+09   \n",
       "1                  1         6           145637.3333        3.567915e+09   \n",
       "2                  1         6           335425.6667        8.291520e+09   \n",
       "3            1006280         6           335426.0000        8.291520e+09   \n",
       "4                  6         6           535187.3333        1.321503e+10   \n",
       "\n",
       "   Packet_Std_Dev  Byte_Std_Dev  Duration_per_Flow  Label  \n",
       "0         24719.5  5.339086e+09           1.258000      0  \n",
       "1         24721.0  5.339086e+09           1.254000      0  \n",
       "2         61289.5  1.240812e+10           2.587333      0  \n",
       "3         61288.0  1.240812e+10           2.592000      0  \n",
       "4         97006.0  1.977597e+10           3.926000      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.dtypes)\n",
    "print(df['Label'].value_counts()[1])\n",
    "print(df['Label'].value_counts()[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e4c5b01-2c83-421a-a31a-dad6da3377ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_cols=df.dtypes[df.dtypes == \"object\"].index.values.tolist()\n",
    "# print(obj_cols)\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# #Encode labels of multiple columns at once\n",
    "\n",
    "# df[obj_cols] = df[obj_cols].astype(str)\n",
    "# df[obj_cols] = df[obj_cols].apply(LabelEncoder().fit_transform)\n",
    "# #\n",
    "# # Print head\n",
    "# #\n",
    "# print(df.dtypes)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a088bc65-ad53-4303-b9ef-b7c911b0b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# import lightgbm as lgb\n",
    "\n",
    "# y = df.Label\n",
    "# X = df.drop(['Label'],axis=1)\n",
    "\n",
    "# # Scale the dataset using StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "# X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# params = {\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'objective': 'binary',\n",
    "#     'metric': 'binary_logloss',\n",
    "#     'num_leaves': 31,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'feature_fraction': 0.9,\n",
    "#     'bagging_fraction': 0.8,\n",
    "#     'bagging_freq': 5,\n",
    "#     'verbose': 0\n",
    "# }\n",
    "\n",
    "\n",
    "# # Setting up 5-fold cross-validation\n",
    "# kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# fold = 0\n",
    "# results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f12cf1-76b3-40cb-a4dc-bf4a0c086b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset normalized using MinMaxScaler.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_score, make_scorer\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, KFold, cross_validate\n",
    "from datetime import datetime\n",
    "\n",
    "y = df.Label\n",
    "X = df.drop(['Label'],axis=1)\n",
    "\n",
    "# Normalize the dataset using MinMaxScaler\n",
    "normalizer = MinMaxScaler()\n",
    "X_normalized = normalizer.fit_transform(X)\n",
    "X = pd.DataFrame(X_normalized, columns=X.columns)\n",
    "print(\"Dataset normalized using MinMaxScaler.\")\n",
    "\n",
    "# Scale the dataset using StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "# X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "X_train_cv, X_unseen_test, y_train_cv, y_unseen_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52e8ce7-88f3-40ce-bf16-704976068283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50262, number of negative: 50538\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2398\n",
      "[LightGBM] [Info] Number of data points in the train set: 100800, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498631 -> initscore=-0.005476\n",
      "[LightGBM] [Info] Start training from score -0.005476\n",
      "[LightGBM] [Info] Number of positive: 50285, number of negative: 50515\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2397\n",
      "[LightGBM] [Info] Number of data points in the train set: 100800, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498859 -> initscore=-0.004563\n",
      "[LightGBM] [Info] Start training from score -0.004563\n",
      "[LightGBM] [Info] Number of positive: 50214, number of negative: 50586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2399\n",
      "[LightGBM] [Info] Number of data points in the train set: 100800, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498155 -> initscore=-0.007381\n",
      "[LightGBM] [Info] Start training from score -0.007381\n",
      "[LightGBM] [Info] Number of positive: 50333, number of negative: 50467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2401\n",
      "[LightGBM] [Info] Number of data points in the train set: 100800, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499335 -> initscore=-0.002659\n",
      "[LightGBM] [Info] Start training from score -0.002659\n",
      "[LightGBM] [Info] Number of positive: 50261, number of negative: 50539\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2399\n",
      "[LightGBM] [Info] Number of data points in the train set: 100800, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498621 -> initscore=-0.005516\n",
      "[LightGBM] [Info] Start training from score -0.005516\n",
      "[LightGBM] [Info] Number of positive: 50285, number of negative: 50515\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2396\n",
      "[LightGBM] [Info] Number of data points in the train set: 100800, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498859 -> initscore=-0.004563\n",
      "[LightGBM] [Info] Start training from score -0.004563\n",
      "[LightGBM] [Info] Number of positive: 50248, number of negative: 50552\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2400\n",
      "[LightGBM] [Info] Number of data points in the train set: 100800, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498492 -> initscore=-0.006032\n",
      "[LightGBM] [Info] Start training from score -0.006032\n",
      "[LightGBM] [Info] Number of positive: 50272, number of negative: 50528\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2398\n",
      "[LightGBM] [Info] Number of data points in the train set: 100800, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498730 -> initscore=-0.005079\n",
      "[LightGBM] [Info] Start training from score -0.005079\n",
      "[LightGBM] [Info] Number of positive: 50298, number of negative: 50502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2395\n",
      "[LightGBM] [Info] Number of data points in the train set: 100800, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498988 -> initscore=-0.004048\n",
      "[LightGBM] [Info] Start training from score -0.004048\n",
      "[LightGBM] [Info] Number of positive: 50291, number of negative: 50509\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2397\n",
      "[LightGBM] [Info] Number of data points in the train set: 100800, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498919 -> initscore=-0.004325\n",
      "[LightGBM] [Info] Start training from score -0.004325\n",
      "excution time:  0:00:11.631380\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store metrics\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "# Initialize LightGBM classifier\n",
    "lgb_clf = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for train_index, test_index in kf.split(X_train_cv):\n",
    "    X_train, X_test = X_train_cv.iloc[train_index], X_train_cv.iloc[test_index]\n",
    "    y_train, y_test = y_train_cv.iloc[train_index], y_train_cv.iloc[test_index]\n",
    "\n",
    "    # Train the LightGBM classifier\n",
    "    lgb_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test fold\n",
    "    y_pred_test = lgb_clf.predict(X_test)\n",
    "    y_pred_train = lgb_clf.predict(X_train)\n",
    "\n",
    "    # Calculate metrics for the test fold\n",
    "    train_accuracies.append(accuracy_score(y_train, y_pred_train))\n",
    "    test_accuracies.append(accuracy_score(y_test, y_pred_test))\n",
    "    precisions.append(precision_score(y_test, y_pred_test))\n",
    "    recalls.append(recall_score(y_test, y_pred_test))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_test))\n",
    "    confusion_matrices.append(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "# Evaluate on unseen test data\n",
    "y_unseen_pred = lgb_clf.predict(X_unseen_test)\n",
    "unseen_accuracy = accuracy_score(y_unseen_test, y_unseen_pred)\n",
    "unseen_precision = precision_score(y_unseen_test, y_unseen_pred)\n",
    "unseen_recall = recall_score(y_unseen_test, y_unseen_pred)\n",
    "unseen_f1 = f1_score(y_unseen_test, y_unseen_pred)\n",
    "unseen_conf_matrix = confusion_matrix(y_unseen_test, y_unseen_pred)\n",
    "\n",
    "end = datetime.now()\n",
    "print(\"excution time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf2b45f2-fbf5-4668-ba7d-655b96b40c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Confusion Matrix:\n",
      "[[5485.   128.9]\n",
      " [  20.7 5565.4]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the average confusion matrix\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an array to store the sum of confusion matrices\n",
    "confusion_matrix_sum = np.zeros((2, 2))  # Adjust the size if multi-class classification\n",
    "\n",
    "# During the cross-validation loop, confusion matrices were already calculated\n",
    "# Here, we'll sum up those matrices (assumes `confusion_matrices` contains all fold matrices)\n",
    "for cm in confusion_matrices:\n",
    "    confusion_matrix_sum += cm\n",
    "\n",
    "# Compute the average confusion matrix\n",
    "average_conf_matrix = confusion_matrix_sum / 10\n",
    "\n",
    "# Display the average confusion matrix\n",
    "print(\"Average Confusion Matrix:\")\n",
    "print(average_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a1e2695-cc6d-4363-a39c-0bc143c6b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Training Accuracy: 0.9878373015873015\n",
      "Testing Accuracy: 0.9873214285714286\n",
      "Precision: 0.9789362822538178\n",
      "Recall: 0.9960707269155207\n",
      "F1-Score: 0.9874291784702549\n",
      "Confusion Matrix:\n",
      "[[5481  120]\n",
      " [  22 5577]]\n",
      "\n",
      "Fold 2:\n",
      "Training Accuracy: 0.9878769841269841\n",
      "Testing Accuracy: 0.9852678571428571\n",
      "Precision: 0.9765721331689272\n",
      "Recall: 0.994261119081779\n",
      "F1-Score: 0.9853372434017595\n",
      "Confusion Matrix:\n",
      "[[5491  133]\n",
      " [  32 5544]]\n",
      "\n",
      "Fold 3:\n",
      "Training Accuracy: 0.987718253968254\n",
      "Testing Accuracy: 0.9878571428571429\n",
      "Precision: 0.9779705117085863\n",
      "Recall: 0.9984062333982646\n",
      "F1-Score: 0.9880827199439187\n",
      "Confusion Matrix:\n",
      "[[5426  127]\n",
      " [   9 5638]]\n",
      "\n",
      "Fold 4:\n",
      "Training Accuracy: 0.9877579365079365\n",
      "Testing Accuracy: 0.9869642857142857\n",
      "Precision: 0.9778053977272727\n",
      "Recall: 0.9962011577424024\n",
      "F1-Score: 0.9869175627240143\n",
      "Confusion Matrix:\n",
      "[[5547  125]\n",
      " [  21 5507]]\n",
      "\n",
      "Fold 5:\n",
      "Training Accuracy: 0.9876388888888888\n",
      "Testing Accuracy: 0.9883035714285714\n",
      "Precision: 0.9799894681411269\n",
      "Recall: 0.9969642857142857\n",
      "F1-Score: 0.9884040010622289\n",
      "Confusion Matrix:\n",
      "[[5486  114]\n",
      " [  17 5583]]\n",
      "\n",
      "Fold 6:\n",
      "Training Accuracy: 0.9878670634920635\n",
      "Testing Accuracy: 0.9878571428571429\n",
      "Precision: 0.9793796263658795\n",
      "Recall: 0.9965925394548063\n",
      "F1-Score: 0.9879111111111111\n",
      "Confusion Matrix:\n",
      "[[5507  117]\n",
      " [  19 5557]]\n",
      "\n",
      "Fold 7:\n",
      "Training Accuracy: 0.9881051587301587\n",
      "Testing Accuracy: 0.9855357142857143\n",
      "Precision: 0.9759036144578314\n",
      "Recall: 0.9957242116515232\n",
      "F1-Score: 0.9857142857142858\n",
      "Confusion Matrix:\n",
      "[[5449  138]\n",
      " [  24 5589]]\n",
      "\n",
      "Fold 8:\n",
      "Training Accuracy: 0.9878373015873015\n",
      "Testing Accuracy: 0.986875\n",
      "Precision: 0.9768664563617245\n",
      "Recall: 0.9973161567364466\n",
      "F1-Score: 0.9869853917662682\n",
      "Confusion Matrix:\n",
      "[[5479  132]\n",
      " [  15 5574]]\n",
      "\n",
      "Fold 9:\n",
      "Training Accuracy: 0.9881547619047619\n",
      "Testing Accuracy: 0.9853571428571428\n",
      "Precision: 0.975850520007051\n",
      "Recall: 0.995146503685062\n",
      "F1-Score: 0.9854040583837664\n",
      "Confusion Matrix:\n",
      "[[5500  137]\n",
      " [  27 5536]]\n",
      "\n",
      "Fold 10:\n",
      "Training Accuracy: 0.9881547619047619\n",
      "Testing Accuracy: 0.9850892857142857\n",
      "Precision: 0.9743634767339772\n",
      "Recall: 0.996229802513465\n",
      "F1-Score: 0.9851753217931647\n",
      "Confusion Matrix:\n",
      "[[5484  146]\n",
      " [  21 5549]]\n",
      "\n",
      "Average Training Accuracy: 0.9878948412698414\n",
      "Average Testing Accuracy: 0.9866428571428572\n",
      "Average Precision: 0.9773637486926194\n",
      "Average recall: 0.9962912736893555\n",
      "Average F1 Score: 0.9867360874370773\n",
      "Performance on Unseen Data:\n",
      "Accuracy: 0.9865357142857143\n",
      "Precision: 0.978112840466926\n",
      "Recall: 0.9956149656977156\n",
      "F1-Score: 0.9867863026182048\n",
      "Confusion Matrix:\n",
      "[[13546   315]\n",
      " [   62 14077]]\n",
      "0.9878948412698414\n",
      "0.9866428571428572\n"
     ]
    }
   ],
   "source": [
    "# Print the metrics for each fold\n",
    "for i in range(10):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"Training Accuracy: {train_accuracies[i]}\")\n",
    "    print(f\"Testing Accuracy: {test_accuracies[i]}\")\n",
    "    print(f\"Precision: {precisions[i]}\")\n",
    "    print(f\"Recall: {recalls[i]}\")\n",
    "    print(f\"F1-Score: {f1_scores[i]}\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrices[i]}\\n\")\n",
    "\n",
    "print(f\"Average Training Accuracy: {sum(train_accuracies) / 10}\")\n",
    "print(f\"Average Testing Accuracy: {sum(test_accuracies) / 10}\")\n",
    "print(f\"Average Precision: {sum(precisions) / 10}\")\n",
    "print(f\"Average recall: {sum(recalls) / 10}\")\n",
    "print(f\"Average F1 Score: {sum(f1_scores) / 10}\")\n",
    "\n",
    "# Print the performance on the unseen data\n",
    "print(\"Performance on Unseen Data:\")\n",
    "print(f\"Accuracy: {unseen_accuracy}\")\n",
    "print(f\"Precision: {unseen_precision}\")\n",
    "print(f\"Recall: {unseen_recall}\")\n",
    "print(f\"F1-Score: {unseen_f1}\")\n",
    "print(f\"Confusion Matrix:\\n{unseen_conf_matrix}\")\n",
    "print(sum(train_accuracies) / 10)\n",
    "print(sum(test_accuracies) / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30a4ea85-cc4d-461e-a291-470f553be27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weighted Feature Importance (WFI):\n",
      "                 Feature  Importance\n",
      "4      Lookup_Count_Diff    0.201333\n",
      "10     Duration_per_Flow    0.141333\n",
      "3      Packet_Count_Diff    0.126000\n",
      "0     Count_of_Source_IP    0.113333\n",
      "6   Average_Packet_Count    0.106667\n",
      "9           Byte_Std_Dev    0.068000\n",
      "2       Pair_Count_Ratio    0.062667\n",
      "8         Packet_Std_Dev    0.061333\n",
      "5               Protocol    0.042667\n",
      "1             Port_Count    0.039333\n",
      "7     Average_Byte_Count    0.037333\n"
     ]
    }
   ],
   "source": [
    "# Calculate Weighted Feature Importance (WFI)\n",
    "feature_importances = lgb_clf.feature_importances_\n",
    "wfi = feature_importances / feature_importances.sum()\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': wfi\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nWeighted Feature Importance (WFI):\")\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edbfdaf1-be14-4b4d-8ef1-aa4930754553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('lgb_binary.pkl', 'wb') as file:\n",
    "#     pickle.dump(lgb_clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a99f2-f5c8-44d8-8cc9-46ec5484a7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
