{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e869096d-6afe-426b-bb90-ba8168f6af8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count of Source IP</th>\n",
       "      <th>Port Count</th>\n",
       "      <th>Pair Count Ratio</th>\n",
       "      <th>Packet Count Diff</th>\n",
       "      <th>Lookup Count Diff</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Average Packet Count</th>\n",
       "      <th>Average Byte Count</th>\n",
       "      <th>Packet Std Dev</th>\n",
       "      <th>Byte Std Dev</th>\n",
       "      <th>Duration per Flow</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>4.788991</td>\n",
       "      <td>0.163601</td>\n",
       "      <td>28.466647</td>\n",
       "      <td>0.668020</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>1.596330</td>\n",
       "      <td>0.095342</td>\n",
       "      <td>16.589551</td>\n",
       "      <td>0.419081</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018792</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.023853</td>\n",
       "      <td>4.150459</td>\n",
       "      <td>0.355016</td>\n",
       "      <td>61.772825</td>\n",
       "      <td>2.946281</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>4.788991</td>\n",
       "      <td>0.163601</td>\n",
       "      <td>28.466647</td>\n",
       "      <td>2.674461</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Count of Source IP  Port Count  Pair Count Ratio  Packet Count Diff  \\\n",
       "0                 545         545               0.0                  2   \n",
       "1                 545         545               0.0                 10   \n",
       "2                  52          52               0.0                  3   \n",
       "3                 545         545               0.0                 13   \n",
       "4                 545         545               0.0                  2   \n",
       "\n",
       "   Lookup Count Diff  Protocol  Average Packet Count  Average Byte Count  \\\n",
       "0                 43         6              0.027523            4.788991   \n",
       "1                  0         6              0.009174            1.596330   \n",
       "2                  0         6              0.000000            0.000000   \n",
       "3                 21         6              0.023853            4.150459   \n",
       "4                131         6              0.027523            4.788991   \n",
       "\n",
       "   Packet Std Dev  Byte Std Dev  Duration per Flow  Label  \n",
       "0        0.163601     28.466647           0.668020      3  \n",
       "1        0.095342     16.589551           0.419081      3  \n",
       "2        0.000000      0.000000           0.018792      3  \n",
       "3        0.355016     61.772825           2.946281      3  \n",
       "4        0.163601     28.466647           2.674461      3  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "df = pd.read_csv('research_traffic_multiple_labels.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41c43163-419c-4c0d-bb63-e74c9bc93147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset normalized using MinMaxScaler.\n",
      "Data normalized and split.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold, cross_validate\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, classification_report\n",
    ")\n",
    "from datetime import datetime\n",
    "\n",
    "y = df.Label\n",
    "X = df.drop(['Label'],axis=1)\n",
    "\n",
    "# Normalize the dataset using MinMaxScaler\n",
    "normalizer = MinMaxScaler()\n",
    "X_normalized = normalizer.fit_transform(X)\n",
    "X = pd.DataFrame(X_normalized, columns=X.columns)\n",
    "print(\"Dataset normalized using MinMaxScaler.\")\n",
    "\n",
    "X_train_cv, X_unseen_test, y_train_cv, y_unseen_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "print(\"Data normalized and split.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5b579f-fda1-459a-8759-99ddcb788135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "1    2056\n",
      "0    2009\n",
      "2    2005\n",
      "3    2004\n",
      "4    1965\n",
      "5    1961\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_unseen_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3483b7f-3904-45cf-9031-1b09dbc0ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(weights):\n",
    "    weighted_predictions = []\n",
    "    for i in range(len(y_unseen_test)):\n",
    "        class_votes = {}\n",
    "        for j, (name, model) in enumerate(models):\n",
    "            pred = predictions[name][i]\n",
    "            class_votes[pred] = class_votes.get(pred, 0) + weights[j]\n",
    "        weighted_predictions.append(max(class_votes, key=class_votes.get))\n",
    "    acc = accuracy_score(y_unseen_test, weighted_predictions)\n",
    "    return (acc,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55c4a178-2e34-4e0b-9778-41d43113ef86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "start = datetime.now()\n",
    "pickle_files = [\"xgb_multi.pkl\", \"lgb_multi.pkl\", \"rf_multi.pkl\", \"ann_multi.pkl\", \"knn_multi.pkl\", \"dt_multi.pkl\", \"nb_multi.pkl\", \"lr_multi.pkl\", \"svm_multi.pkl\"]\n",
    "for filename in pickle_files:\n",
    "    with open(filename, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    model_name = filename.replace(\"_multi.pkl\", \"\")\n",
    "    models.append((model_name, model))\n",
    "print(f\"{len(models)} models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6d1058d-b6a4-464a-b1ec-4d7cb6363a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model: xgb\n",
      "Accuracy:  0.9550833333333333\n",
      "Precision: 0.9570352162582053\n",
      "Recall:    0.9550833333333333\n",
      "F1 Score:  0.9538837766484161\n",
      "Confusion Matrix:\n",
      "[[1569   82    1   88    2  267]\n",
      " [  12 2021    0    0    0   23]\n",
      " [   1    1 2003    0    0    0]\n",
      " [   5    0    0 1999    0    0]\n",
      " [   3    1    0    0 1961    0]\n",
      " [  40   13    0    0    0 1908]]\n",
      "----------------------------------------\n",
      "Results for model: lgb\n",
      "Accuracy:  0.95475\n",
      "Precision: 0.9565394471248935\n",
      "Recall:    0.95475\n",
      "F1 Score:  0.9535446748169473\n",
      "Confusion Matrix:\n",
      "[[1569   82    2   90    2  264]\n",
      " [  12 2021    0    0    0   23]\n",
      " [   2    0 2003    0    0    0]\n",
      " [   7    0    0 1997    0    0]\n",
      " [   5    0    0    0 1960    0]\n",
      " [  41   13    0    0    0 1907]]\n",
      "----------------------------------------\n",
      "Results for model: rf\n",
      "Accuracy:  0.9436666666666667\n",
      "Precision: 0.944487940151616\n",
      "Recall:    0.9436666666666667\n",
      "F1 Score:  0.9424029494507786\n",
      "Confusion Matrix:\n",
      "[[1529   86    0  102    2  290]\n",
      " [  21 1993    0    0    0   42]\n",
      " [   3    0 2002    0    0    0]\n",
      " [   0    0    0 2004    0    0]\n",
      " [   6    0    0    0 1959    0]\n",
      " [ 111   13    0    0    0 1837]]\n",
      "----------------------------------------\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Results for model: ann\n",
      "Accuracy:  0.9333333333333333\n",
      "Precision: 0.9397747355631773\n",
      "Recall:    0.9333333333333333\n",
      "F1 Score:  0.9311059999210164\n",
      "Confusion Matrix:\n",
      "[[1373   93    0  102    1  440]\n",
      " [  52 1927    0    0    0   77]\n",
      " [   7    0 1998    0    0    0]\n",
      " [   2    0    0 2001    0    1]\n",
      " [  15    0    0    0 1950    0]\n",
      " [  10    0    0    0    0 1951]]\n",
      "----------------------------------------\n",
      "Results for model: knn\n",
      "Accuracy:  0.923\n",
      "Precision: 0.9226074100543392\n",
      "Recall:    0.923\n",
      "F1 Score:  0.922107217624306\n",
      "Confusion Matrix:\n",
      "[[1478   98    1  101    2  329]\n",
      " [  60 1933    0    0    0   63]\n",
      " [   5    0 2000    0    0    0]\n",
      " [  25    0    0 1979    0    0]\n",
      " [  11    0    0    0 1954    0]\n",
      " [ 215   14    0    0    0 1732]]\n",
      "----------------------------------------\n",
      "Results for model: dt\n",
      "Accuracy:  0.9224166666666667\n",
      "Precision: 0.9224408102778786\n",
      "Recall:    0.9224166666666667\n",
      "F1 Score:  0.9224207225671108\n",
      "Confusion Matrix:\n",
      "[[1567   86    6   86    5  259]\n",
      " [  94 1935    0    0    0   27]\n",
      " [   2    2 2000    1    0    0]\n",
      " [  97    0    0 1907    0    0]\n",
      " [   3    2    0    0 1960    0]\n",
      " [ 234   25    1    1    0 1700]]\n",
      "----------------------------------------\n",
      "Results for model: nb\n",
      "Accuracy:  0.8985\n",
      "Precision: 0.910106404608627\n",
      "Recall:    0.8985\n",
      "F1 Score:  0.8870019644400279\n",
      "Confusion Matrix:\n",
      "[[ 939  422   25  102   89  432]\n",
      " [   7 1967    0    0    5   77]\n",
      " [   5    3 1997    0    0    0]\n",
      " [   2   16    0 1986    0    0]\n",
      " [  11    2    0    0 1952    0]\n",
      " [  11    9    0    0    0 1941]]\n",
      "----------------------------------------\n",
      "Results for model: lr\n",
      "Accuracy:  0.9193333333333333\n",
      "Precision: 0.9237362877452645\n",
      "Recall:    0.9193333333333333\n",
      "F1 Score:  0.9161954992693279\n",
      "Confusion Matrix:\n",
      "[[1291   94   22  117   20  465]\n",
      " [  50 1917    0    0   12   77]\n",
      " [   7    1 1997    0    0    0]\n",
      " [  57    0    0 1947    0    0]\n",
      " [  16    1    0    0 1948    0]\n",
      " [  29    0    0    0    0 1932]]\n",
      "----------------------------------------\n",
      "Results for model: svm\n",
      "Accuracy:  0.9264166666666667\n",
      "Precision: 0.9324266492895448\n",
      "Recall:    0.9264166666666667\n",
      "F1 Score:  0.9236319936006778\n",
      "Confusion Matrix:\n",
      "[[1329   93    0  118   24  445]\n",
      " [  54 1914    0    0   11   77]\n",
      " [  17    0 1988    0    0    0]\n",
      " [   5    0    0 1990    0    9]\n",
      " [  15    0    0    0 1950    0]\n",
      " [  15    0    0    0    0 1946]]\n",
      "----------------------------------------\n",
      "Predictions obtained from all models.\n",
      "Majority Vote Ensemble Evaluation (Pandas Mode):\n",
      "Accuracy:  0.9439166666666666\n",
      "Precision: 0.9496062141765884\n",
      "Recall:    0.9439166666666666\n",
      "F1 Score:  0.9420862132189579\n",
      "Confusion Matrix:\n",
      " [[1447   90    1  103    2  366]\n",
      " [  23 1965    0    0    0   68]\n",
      " [   5    0 2000    0    0    0]\n",
      " [   1    0    0 2003    0    0]\n",
      " [   7    0    0    0 1958    0]\n",
      " [   5    2    0    0    0 1954]]\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "for name, model in models:\n",
    "    if name == \"ann\":\n",
    "        prediction = np.array((model.predict(X_unseen_test) > 0.5).astype(int))\n",
    "    else:\n",
    "        prediction = np.array(model.predict(X_unseen_test))\n",
    "    if prediction.ndim > 1 and prediction.shape[1] > 1:  # if not 1-dimensional, flatten it\n",
    "        prediction = np.argmax(prediction, axis=1)\n",
    "    elif prediction.ndim > 1:\n",
    "        prediction = prediction.ravel()\n",
    "    predictions[name] = prediction\n",
    "\n",
    "    acc = accuracy_score(y_unseen_test, prediction)\n",
    "    prec = precision_score(y_unseen_test, prediction, average='weighted')\n",
    "    rec = recall_score(y_unseen_test, prediction, average='weighted')\n",
    "    f1  = f1_score(y_unseen_test, prediction, average='weighted')\n",
    "    cm  = confusion_matrix(y_unseen_test, prediction)\n",
    "\n",
    "    # Print the results for the current model\n",
    "    print(f\"Results for model: {name}\")\n",
    "    print(\"Accuracy: \", acc)\n",
    "    print(\"Precision:\", prec)\n",
    "    print(\"Recall:   \", rec)\n",
    "    print(\"F1 Score: \", f1)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"-\" * 40)\n",
    "# # Optional: Inspect one of the predictions\n",
    "# print(predictions['ann'])\n",
    "# Step 2: Combine predictions and apply majority vote ensemble using pandas mode\n",
    "# Create a DataFrame where each column corresponds to a model's predictions\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "\n",
    "print(\"Predictions obtained from all models.\")\n",
    "\n",
    "# For each sample (row), get the mode (most common value) across the classifiers.\n",
    "# Since there are 9 classifiers (an odd number), ties should not occur.\n",
    "ensemble_predictions = pred_df.mode(axis=1)[0].values\n",
    "\n",
    "# Step 3: Evaluate the ensemble predictions using standard metrics\n",
    "accuracy  = accuracy_score(y_unseen_test, ensemble_predictions)\n",
    "precision = precision_score(y_unseen_test, ensemble_predictions, average='weighted')\n",
    "recall    = recall_score(y_unseen_test, ensemble_predictions, average='weighted')\n",
    "f1        = f1_score(y_unseen_test, ensemble_predictions, average='weighted')\n",
    "cm        = confusion_matrix(y_unseen_test, ensemble_predictions)\n",
    "\n",
    "# Print out the results\n",
    "print(\"Majority Vote Ensemble Evaluation (Pandas Mode):\")\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:   \", recall)\n",
    "print(\"F1 Score: \", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd437414-1452-454e-a189-cd572c2c17a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t50    \n",
      "1  \t28    \n",
      "2  \t38    \n",
      "3  \t34    \n",
      "4  \t29    \n",
      "5  \t24    \n",
      "6  \t35    \n",
      "7  \t30    \n",
      "8  \t35    \n",
      "9  \t28    \n",
      "10 \t26    \n",
      "11 \t30    \n",
      "12 \t42    \n",
      "13 \t37    \n",
      "14 \t31    \n",
      "15 \t33    \n",
      "16 \t23    \n",
      "17 \t36    \n",
      "18 \t32    \n",
      "19 \t33    \n",
      "20 \t28    \n",
      "21 \t34    \n",
      "22 \t30    \n",
      "23 \t32    \n",
      "24 \t23    \n",
      "25 \t28    \n",
      "26 \t28    \n",
      "27 \t21    \n",
      "28 \t36    \n",
      "29 \t32    \n",
      "30 \t30    \n",
      "31 \t31    \n",
      "32 \t34    \n",
      "33 \t28    \n",
      "34 \t32    \n",
      "35 \t26    \n",
      "36 \t31    \n",
      "37 \t35    \n",
      "38 \t32    \n",
      "39 \t36    \n",
      "40 \t32    \n",
      "41 \t25    \n",
      "42 \t32    \n",
      "43 \t14    \n",
      "44 \t27    \n",
      "45 \t26    \n",
      "46 \t25    \n",
      "47 \t33    \n",
      "48 \t30    \n",
      "49 \t36    \n",
      "50 \t24    \n",
      "Optimized Classifier Weights: {'xgb': 0.47082968863626756, 'lgb': 0.3037283814863955, 'rf': 0.06867431323887492, 'ann': -0.16007424893521638, 'knn': -0.10914527642860268, 'dt': 0.2715298327978744, 'nb': 0.06768461737260743, 'lr': 0.18724737269604633, 'svm': -0.10047468086424723}\n"
     ]
    }
   ],
   "source": [
    "# Set up the Genetic Algorithm\n",
    "num_models = len(models)\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Maximize accuracy\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Define population and individual initialization\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_float\", np.random.rand)  # Random weight initialization\n",
    "\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=num_models)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Genetic operators\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.2, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Run Genetic Algorithm\n",
    "pop = toolbox.population(n=50)  # Population size\n",
    "algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=50, verbose=True)\n",
    "\n",
    "# Extract best weights\n",
    "best_individual = np.array(tools.selBest(pop, k=1)[0])\n",
    "optimized_weights = best_individual / np.sum(best_individual)  # Normalize weights\n",
    "\n",
    "# Store classifier names with optimized weights in a dictionary\n",
    "classifier_names = [name for name, _ in models]\n",
    "classifier_weights = {clf_name: weight for clf_name, weight in zip(classifier_names, optimized_weights)}\n",
    "\n",
    "print(\"Optimized Classifier Weights:\", classifier_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e27f673-22be-4853-a572-78418dc0d1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Ensemble Evaluation:\n",
      "Accuracy:  0.9555833333333333\n",
      "Precision: 0.9575773893194756\n",
      "Recall:    0.9555833333333333\n",
      "F1 Score:  0.9543744387128484\n",
      "Confusion Matrix:\n",
      " [[1570   81    1   90    2  265]\n",
      " [  10 2024    0    0    0   22]\n",
      " [   1    1 2003    0    0    0]\n",
      " [   4    0    0 2000    0    0]\n",
      " [   3    1    0    0 1961    0]\n",
      " [  39   13    0    0    0 1909]]\n",
      "training time:  0:02:36.848713\n"
     ]
    }
   ],
   "source": [
    "# List to store the final weighted ensemble predictions\n",
    "weighted_ensemble_predictions = []\n",
    "n_samples = len(y_unseen_test)\n",
    "\n",
    "# Loop over each sample in the test set\n",
    "for i in range(n_samples):\n",
    "    weighted_votes = {}\n",
    "\n",
    "    for name, prediction in predictions.items():\n",
    "        pred_class = prediction[i]\n",
    "        weight = classifier_weights.get(name, 1.0)\n",
    "        # Accumulate the vote for the predicted class\n",
    "        weighted_votes[pred_class] = weighted_votes.get(pred_class, 0) + weight\n",
    "\n",
    "    final_class = max(weighted_votes, key=weighted_votes.get)\n",
    "    weighted_ensemble_predictions.append(final_class)\n",
    "\n",
    "accuracy  = accuracy_score(y_unseen_test, weighted_ensemble_predictions)\n",
    "precision = precision_score(y_unseen_test, weighted_ensemble_predictions, average='weighted')\n",
    "recall    = recall_score(y_unseen_test, weighted_ensemble_predictions, average='weighted')\n",
    "f1        = f1_score(y_unseen_test, weighted_ensemble_predictions, average='weighted')\n",
    "cm        = confusion_matrix(y_unseen_test, weighted_ensemble_predictions)\n",
    "\n",
    "end = datetime.now()\n",
    "\n",
    "print(\"Weighted Ensemble Evaluation:\")\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:   \", recall)\n",
    "print(\"F1 Score: \", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"training time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77f0b26a-1598-440a-83ae-a01357c5085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred = pd.read_csv('multiple_test.csv')\n",
    "\n",
    "# y_pred = df_pred.Label\n",
    "# X_pred = df_pred.drop(['Label'],axis=1)\n",
    "\n",
    "# X_pred_normalized = normalizer.fit_transform(X_pred)\n",
    "# X_pred = pd.DataFrame(X_pred_normalized, columns=X_pred.columns)\n",
    "# print(\"Dataset normalized using MinMaxScaler.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02719d45-a3f5-46a3-9302-8046d8bc0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_test = {}\n",
    "# for name, model in models:\n",
    "#     if name == \"ann\":\n",
    "#         prediction= np.array((model.predict(X_pred) > 0.5).astype(int))\n",
    "#     else:\n",
    "#         prediction = np.array(model.predict(X_pred))\n",
    "#     if prediction.ndim > 1 and prediction.shape[1] > 1:  # if not 1-dimensional, flatten it\n",
    "#         prediction = np.argmax(prediction, axis=1)\n",
    "#     elif prediction.ndim > 1:\n",
    "#         prediction = prediction.ravel()\n",
    "#     predictions_test[name] = prediction\n",
    "\n",
    "#     acc = accuracy_score(y_pred, prediction)\n",
    "#     prec = precision_score(y_pred, prediction, average='weighted')\n",
    "#     rec = recall_score(y_pred, prediction, average='weighted')\n",
    "#     f1  = f1_score(y_pred, prediction, average='weighted')\n",
    "#     cm  = confusion_matrix(y_pred, prediction)\n",
    "\n",
    "#     # Print the results for the current model\n",
    "#     print(f\"Results for model: {name}\")\n",
    "#     print(\"Accuracy: \", acc)\n",
    "#     print(\"Precision:\", prec)\n",
    "#     print(\"Recall:   \", rec)\n",
    "#     print(\"F1 Score: \", f1)\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(cm)\n",
    "#     print(\"-\" * 40)\n",
    "# # Optional: Inspect one of the predictions\n",
    "# # print(predictions['ann'])\n",
    "# # Step 2: Combine predictions and apply majority vote ensemble using pandas mode\n",
    "# # Create a DataFrame where each column corresponds to a model's predictions\n",
    "# X_meta_test = pd.DataFrame(predictions_test)\n",
    "\n",
    "# print(\"Predictions obtained from all models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "601f8a1f-943b-4831-9be0-cfddb79f062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# X_meta = pd.DataFrame(predictions)  # Features: predictions from classifiers\n",
    "# y_meta = y_unseen_test  # True labels\n",
    "\n",
    "# # Step 5: Train xgb as a Meta-Classifier\n",
    "# meta_clf = xgb.XGBClassifier(objective='multi:softmax', num_class=6, eval_metric='mlogloss', use_label_encoder=False)\n",
    "# meta_clf.fit(X_meta, y_meta)\n",
    "\n",
    "# # Step 6: Use Meta-Classifier to Make Final Predictions\n",
    "# meta_predictions = meta_clf.predict(X_meta_test)\n",
    "\n",
    "# # Step 7: Evaluate the Meta-Classifier Performance\n",
    "# meta_accuracy = accuracy_score(y_pred, meta_predictions)\n",
    "# meta_precision = precision_score(y_pred, meta_predictions, average='weighted')\n",
    "# meta_recall = recall_score(y_pred, meta_predictions, average='weighted')\n",
    "# meta_f1 = f1_score(y_pred, meta_predictions, average='weighted')\n",
    "# meta_cm = confusion_matrix(y_pred, meta_predictions)\n",
    "\n",
    "# # Print Results\n",
    "# print(\"Meta-Classifier (xgb) Performance:\")\n",
    "# print(f\"Accuracy: {meta_accuracy:.4f}\")\n",
    "# print(f\"Precision: {meta_precision:.4f}\")\n",
    "# print(f\"Recall: {meta_recall:.4f}\")\n",
    "# print(f\"F1 Score: {meta_f1:.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", meta_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46f704-03c0-4606-b2b6-e1b703a4e25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
